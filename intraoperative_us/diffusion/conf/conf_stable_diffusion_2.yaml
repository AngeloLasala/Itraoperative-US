dataset_params:
  # parent directory of the dataset
  # local path: 
  #            1) "/media/angelo/OS/Users/lasal/OneDrive - Scuola Superiore Sant'Anna/PhD_notes/Visiting_Imperial/RESECT_iUS_dataset/dataset"
  #            2) cineca: '/leonardo_work/IscrC_AIM-ORAL/Angelo/ius_data/dataset'
  dataset_path: "/leonardo_work/IscrC_AIM-ORAL/Angelo/ius_data/dataset"  
  im_channels : 1
  im_size_h : 256         # 256
  im_size_w : 256         # 256 
  splitting_seed: 42      # sedd for set reprodicitbilityin random splitting
  train_percentage: 0.8              # percentage of the dataset used for training
  val_percentage: 0.1                # percentage of the dataset used for validation
  test_percentage: 0.1               # percentage of the dataset used for testing
  splitting_json: "splitting.json"   # FOR REPRODUCIBILITY: name of the json file containing the splitting


autoencoder_params:
   ## 'scratch' -> old version,
   ## '/home/angelo/Documenti/Itraoperative-US/intraoperative_us/diffusion/models/vae/AutoencoderKL_SD1.5_default' -> architecture from huggingface
   ## cineca: '/leonardo_work/IscrC_AIM-ORAL/Angelo/Itraoperative-US/intraoperative_us/diffusion/models/vae/AutoencoderKL_SD1.5_default'
   ## 'scratch -> old version'
  autoencoder_type: '/leonardo_work/IscrC_AIM-ORAL/Angelo/Itraoperative-US/intraoperative_us/diffusion/models/vae/AutoencoderKL_SD1.5_default'         
  ## 'random' -> random initialization, 
  ## 'SD1.5'  -> SD1.5 initialization from hugginface
  ## 'only_D' -> E from SD1.5 and D 
  initialization: 'random'
  z_channels: 4            
  codebook_size : 20
  down_channels : [64, 128, 256, 256]      # hf original [128, 256, 512, 512] - small [64, 128, 256, 256]
  mid_channels : [256, 256]
  down_sample : [True, True, True]
  attn_down : [False, False, False]
  norm_channels: 32
  num_heads: 16
  num_down_layers : 2
  num_mid_layers : 2
  num_up_layers : 2
  condition_config:
    condition_types: []                     # could be also ['class', 'text', 'image' ]: PUT THE DEFAULT VALUE AS 'image'
    text_condition_config:
      text_embed_model: 'clip'
      train_text_embed_model: False
      text_embed_dim: 512
      cond_drop_prob: 0.1
    image_condition_config:                
      image_condition_input_channels:  1    
      image_condition_output_channels: 3
      image_condition_h : 256
      image_condition_w : 256
      cond_drop_prob: 0.1
    class_condition_config :               
      num_classes : 4                     
      cond_drop_prob : 0.1                 

diffusion_params:
  # NOTE that the scheduler is only for the sampling
  # we fic the DDPM scheduler for the training
  ## 'linear' -> my linear scheduler,
  ## 'ddim'   -> DDIM scheduler form hugginface - ONLY for sampling
  ## 'pndm'   -> PNDM scheduler from hugginface - ONLY for sampling
  scheduler: 'ddpm'
  ## local path: '/home/angelo/Documenti/Itraoperative-US/intraoperative_us/diffusion/scheduler/'
  ## cineca: '/leonardo_work/IscrC_AIM-ORAL/Angelo/Itraoperative-US/intraoperative_us/diffusion/scheduler/'
  scheduler_path: /leonardo_work/IscrC_AIM-ORAL/Angelo/Itraoperative-US/intraoperative_us/diffusion/scheduler/
  prediction_type: 'epsilon'     # v_prediction, epsilon
  num_sample_timesteps : 1000
  num_train_timesteps : 1000
  beta_start : 0.0015
  beta_end : 0.0195
  lr_scheduler: "constant"             # The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'"constant", "constant_with_warmup"
  lr_warmup_steps: 500                # Number of steps for the warmup in the lr scheduler

ldm_params:
  unet: unet_cond/UNet2DConditionModel_SD1.5_default     ## path of hugginface model
  ## local : "/home/angelo/Documenti/Itraoperative-US/intraoperative_us/diffusion/models"
  ## cineca: "/leonardo_work/IscrC_AIM-ORAL/Angelo/Itraoperative-US/intraoperative_us/diffusion/models"
  unet_path: "/leonardo_work/IscrC_AIM-ORAL/Angelo/Itraoperative-US/intraoperative_us/diffusion/models"
  ## 'random' -> random initialization
  ## 'SD1.5'  -> SD1.5 initialization from hugginface - for exstensive fine-tuning
  initialization: 'SD1.5'
  # in_channels: 4,  i.e. z_channels
  # out_channels: 4, i.e. z_channels
  sample_size: 32                           ## shape of the input of autoencoder
  z_channels: 4                             ## z_channels of the autoencoder  
  down_channels: [320, 640, 1280, 1280]      ## hf original [320, 640, 1280, 1280] - small [128, 256, 256, 256]
  mid_channels: [ 256, 256 ]
  down_sample: [ True, True, True]
  attn_down : [True, True, True]
  time_emb_dim: 256                         ## hf encoder_hid_dim 256 - small 256
  norm_channels: 32                         ## hf original 32 - small 32
  num_heads: 8                              ## hf num_heads 8 - small 8
  conv_out_channels : 128
  num_down_layers : 2                       ## hf leyers_per_block 2
  num_mid_layers : 2
  num_up_layers : 2
  cross_attention_dim : 768
  condition_config:                         ## conditional type is both for CFG and ControlNET
    condition_types: ['image']                         ## dropout probability - CFG > 0.0, (original) ControlNET = 0.0
    image_condition_config:        
      image_condition_input_channels: 1     # total number of spatial channels in the segmentation maps
      image_condition_output_channels: 4
      image_condition_h : 256
      image_condition_w : 256
      cond_drop_prob: 0.1                   ## dropout probability - CFG > 0.0, (original) ControlNET = 0.0
  tokenizer: 'tokenizer/CLIPTokenizer'                 ## tokenizer for the text conditioning
  text_encoder: 'text_encoder/CLIPTextModel'           ## text encoder for the text conditioning
  image_processor: 'image_processor/CLIPImageModel'    ## image encoder for the image conditioning 
  clip_vision_model: 'clip_vision_model/CLIPVisionModel' ## vision model for the image conditioning

train_params:
  seed : 1111
  ldm_batch_size: 16
  ldm_batch_size_sample: 32
  autoencoder_batch_size: 8   # 2: local for shape (240,320) - 4 or 8: cineca for shape (240, 320)
  disc_start: 8
  disc_weight: 0.5
  codebook_weight: 1
  commitment_beta: 0.2
  perceptual_weight: 1
  kl_weight: 0.000005
  ldm_epochs : 9000
  autoencoder_epochs : 100
  num_samples : 25
  num_grid_rows : 5
  ldm_lr: 0.00001
  autoencoder_lr: 0.0001
  autoencoder_acc_steps : 1
  autoencoder_img_save_steps : 8
  save_frequency : 1500
  mixed_precision : 'fp16'
  gradient_accumulation_steps: 8